# -*- coding: utf-8 -*-
"""“

Automatically generated by Colab.
Before running anything, Runtime -> Change Runtime Type -> T4 GPU. This will speed up the results. 
Just run each cell one by one to generate the Drake lyrics!
"""

import torch
import torch.nn as nn
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

"""No need to touch the GPT class below. Just run."""

class GPT(nn.Module):

    class TransformerBlock(nn.Module):

        class MultiHeadedSelfAttention(nn.Module):

            class SingleHeadAttention(nn.Module):
                '''AttentionNet(Q,K,V)= Score_a/sqrt(QK_space_dim)*V
                Score_a = Q*K^T is attention score
                sqrt(QK_space_dim) for value balance
                V = Matrix_value * Embedding  is the change of the semantic
                Matrix_value = M_value_up * M_value_down is a decomposition to reduce the num of parameter
                The M_value_up(s) of all SingleHeadAttention blocks are concatenated in MultiHeadedSelfAttentionClass, and operated parallelly.
                '''
                def __init__(self, embedding_dim: int, QK_space_dim: int):
                    super().__init__()
                    self.key_layer = nn.Linear(embedding_dim, QK_space_dim, bias=False)
                    self.query_layer = nn.Linear(embedding_dim, QK_space_dim, bias=False)
                    self.value_down_layer = nn.Linear(embedding_dim, QK_space_dim, bias=False)#W_down of this head

                def forward(self, embedded):
                    k = self.key_layer(embedded)
                    q = self.query_layer(embedded)
                    v = self.value_down_layer(embedded)

                    scores = q @ torch.transpose(k, 1, 2) # @ is the same as torch.matmul()
                    max_context_length, attention_dim = k.shape[1], k.shape[2]
                    scores = scores / (attention_dim ** 0.5)

                    lower_triangular = torch.tril(torch.ones(max_context_length, max_context_length))
                    mask = (lower_triangular == 0).to(device)
                    scores = scores.masked_fill(mask, float('-inf'))
                    scores = nn.functional.softmax(scores, dim = 2)

                    return scores @ v

            def __init__(self, embedding_dim: int, num_heads: int):
                super().__init__()
                self.attention_heads = nn.ModuleList()
                for i in range(num_heads):
                    self.attention_heads.append(self.SingleHeadAttention(embedding_dim, embedding_dim // num_heads))
                self.value_up_layers = nn.Linear(embedding_dim, embedding_dim)#W_up of all heads
                self.dropout = nn.Dropout(0.2)

            def forward(self, embedded):
                head_outputs = []
                for head in self.attention_heads:
                    head_outputs.append(head(embedded))
                concatenated = torch.cat(head_outputs, dim = 2)
                return self.dropout(self.value_up_layers(concatenated)) # context_token_num * embedding_dim

        class VanillaNeuralNetwork(nn.Module):

            def __init__(self, embedding_dim: int):
                super().__init__()
                self.first_linear_layer = nn.Linear(embedding_dim, embedding_dim * 4)
                self.relu = nn.ReLU()
                self.second_linear_layer = nn.Linear(embedding_dim * 4, embedding_dim)
                self.dropout = nn.Dropout(0.2) # using p = 0.2

            def forward(self, x):
                return self.dropout(self.second_linear_layer(self.relu(self.first_linear_layer(x))))

        def __init__(self, embedding_dim: int, num_heads: int):
            super().__init__()
            self.MHSA = self.MultiHeadedSelfAttention(embedding_dim, num_heads)
            self.vanilla_nn = self.VanillaNeuralNetwork(embedding_dim)
            self.layer_norm_one = nn.LayerNorm(embedding_dim)
            self.layer_norm_two = nn.LayerNorm(embedding_dim)

        def forward(self, embedded):
            embedded = embedded + self.MHSA(self.layer_norm_one(embedded)) # skip connection
            embedded = embedded + self.vanilla_nn(self.layer_norm_two(embedded)) # another skip connection
            return embedded

    def __init__(self, token_dict_size: int, max_context_length: int, embedding_dim: int, num_blocks: int, num_heads: int):
        super().__init__()
        self.token_embedding = nn.Embedding(token_dict_size, embedding_dim)
        self.pos_embedding = nn.Embedding(max_context_length, embedding_dim)
        self.transformer_blocks = nn.Sequential()
        for i in range(num_blocks):
            self.transformer_blocks.append(self.TransformerBlock(embedding_dim, num_heads))
        self.layer_norm_three = nn.LayerNorm(embedding_dim)
        self.vocab_projection = nn.Linear(embedding_dim, token_dict_size)

    def forward(self, context):
        embedded = self.token_embedding(context)
        max_context_length = context.shape[1]
        positions = torch.arange(max_context_length).to(device)
        embedded = embedded + self.pos_embedding(positions)

        raw_output = self.vocab_projection(self.layer_norm_three(self.transformer_blocks(embedded)))
        # batch, max_context_length, token_dict_size

        return raw_output

"""Your generate() function:"""

def generate(model, new_chars: int, context, max_context_length: int, token_dict: dict) -> str:
    res = [token_dict[k.item()] for k in context[0]]
    for i in range(new_chars):
        if len(context.T) > max_context_length:
            context = context[:, -max_context_length:]
        prediction = model(context) # Batch, context_token_num, token_dict_size: 1, 4, 104
        last_time_step = prediction[:, -1, :] # Batch, token_dict_size : 1, 104
        probabilities = nn.functional.softmax(last_time_step, dim = -1)
        next_char = torch.multinomial(probabilities, 1)
        context = torch.cat((context, next_char), dim = -1)
        res.append(token_dict[next_char.item()])
    return ''.join(res)

"""Define the hyperparameters, instantiate the model, and load in the weights from training."""

class userdict(dict):
    def get_key(self,val):
        for key, value in self.items():
            if val == value:
                return key
        return "key doesn't exist"


token_dict = {0: '\n', 1: ' ', 2: '!', 3: '"', 4: '$', 5: '%', 6: '&', 7: "'", 8: '(', 9: ')', 10: '*', 11: '+', 12: ',', 13: '-', 14: '.', 15: '/', 16: '0', 17: '1', 18: '2', 19: '3', 20: '4', 21: '5', 22: '6', 23: '7', 24: '8', 25: '9', 26: ':', 27: ';', 28: '?', 29: 'A', 30: 'B', 31: 'C', 32: 'D', 33: 'E', 34: 'F', 35: 'G', 36: 'H', 37: 'I', 38: 'J', 39: 'K', 40: 'L', 41: 'M', 42: 'N', 43: 'O', 44: 'P', 45: 'Q', 46: 'R', 47: 'S', 48: 'T', 49: 'U', 50: 'V', 51: 'W', 52: 'X', 53: 'Y', 54: 'Z', 55: '[', 56: ']', 57: '_', 58: 'a', 59: 'b', 60: 'c', 61: 'd', 62: 'e', 63: 'f', 64: 'g', 65: 'h', 66: 'i', 67: 'j', 68: 'k', 69: 'l', 70: 'm', 71: 'n', 72: 'o', 73: 'p', 74: 'q', 75: 'r', 76: 's', 77: 't', 78: 'u', 79: 'v', 80: 'w', 81: 'x', 82: 'y', 83: 'z', 84: '{', 85: '|', 86: '}', 87: 'à', 88: 'á', 89: 'è', 90: 'é', 91: 'ë', 92: 'ñ', 93: 'ó', 94: 'ú', 95: '\u2005', 96: '–', 97: '—', 98: '‘', 99: '’', 100: '“', 101: '”', 102: '…', 103: '\u205f'}
token_dict = userdict(token_dict)
token_dict_size = len(token_dict)
max_context_length = 128
embedding_dim = 252
num_blocks = 6
num_heads = 6
assert embedding_dim % num_heads == 0

model = GPT(token_dict_size, max_context_length, embedding_dim, num_blocks, num_heads).to(device)
WEIGHT_PATH = 'weights_lyrics.pt' # Adjust as necessary
model.load_state_dict(torch.load(WEIGHT_PATH))
model.eval()
""" Set the number of characters and prompt you want """
new_chars = 1500 # customized by yourself
prompt_text = "I a" # customized by yourself
prompt = torch.tensor([[token_dict.get_key(t) for t in prompt_text]], dtype = torch.int64).to(device)#convert character to index number [[37,1,58]] of nn.Embedding


"""Run and get lyrics!"""

print(generate(model, new_chars,prompt,
               max_context_length,
               token_dict))


# I always for but let I do
# They gon' need to sometion I'm so I'm so birvin'
# I'll still you miss I know, what you baby?

# [Chorus: Drake]
# Ayy, look I never loved loved up
# You been someone thone is earlouble
# I need your ylowing right now?
# Oh, you talking comerolls by right, huh? Yeah
# Hey, hahahaha, ho, ohhh, oh, oh, ohh
# You not one, ayy, eah"
# "[ Drake 1]
# Ayy, ayy, ayy, ayy ayy, ayy, ayy, ayy, ayy

# [Verse 3: Drake]
# Huh, you nobody, and get it
# Your nobody one, on, oh, oh, oh, oh, you oh, ayy, you
# I've be
